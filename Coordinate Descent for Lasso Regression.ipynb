{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate Descent and Soft-Thresholding for Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n",
    "This notebook explores these topics on Lasso Regression\n",
    "- Solve lasso regression with orthogonal design matrix with soft-thresholding\n",
    "- Solve multivariate lasso regression using coordinate descent when the design matrix is properly scaled\n",
    "- Re-scale the multivariate lasso regression solution back onto the orignal scale of the data  \n",
    "\n",
    "This notebook requires some prior knowlege about regression analysis and lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Soft-thresholding a lasso problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classical regression problem, the assumption placed on data is:  \n",
    "$$y = X\\beta + \\epsilon$$  \n",
    "Where $y$ is a vector of size $(n, 1)$, $X$ a matrix of size $(n, p)$, $\\beta$ a vector of size $(p, 1)$, and $\\epsilon$ a vector of size $(n, 1)$. Also, $\\epsilon \\sim N(0, \\sigma^2)$  \n",
    "Define the loss function of lasso regression:  \n",
    "\n",
    "$$\\frac{1}{2n}|| y - X\\beta ||^2 = \\frac{1}{2n}|| y - X\\hat{\\beta}^{OLS} + X\\hat{\\beta}^{OLS} - X\\beta||^2 + \\lambda|| \\beta ||^1$$  \n",
    "  \n",
    "$$\\hspace{3.2cm}=\\frac{1}{2n}|| y - X\\hat{\\beta}^{OLS}||^2 + || X\\hat{\\beta}^{OLS} - X\\beta||^2+ \\lambda|| \\beta ||^1$$  \n",
    "Where the cross product term:\n",
    "$$2(y - X\\hat{\\beta}^{OLS})^T( X\\hat{\\beta}^{OLS} - X\\beta)=2r^{T}(X\\hat{\\beta}^{OLS} - X\\beta)=0$$  \n",
    "\n",
    "\n",
    "because the second term is on the column space of $X$, which is orthogonal to $r$, the residual of OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since $||y - X\\hat{\\beta}^{OLS}||^2$ is not a function of $\\beta$, the loss function is minimized by minimizing $||X\\hat{\\beta}^{OLS} - X\\beta||^2+ \\lambda||\\beta||^1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:  \n",
    "$$\\hat{\\beta}^{lasso}=\\frac{1}{2n}||X\\hat{\\beta}^{OLS} - X\\beta||^2+ \\lambda||\\beta||^1$$  \n",
    "\n",
    "$$\\hspace{5.8cm}=argmin\\hspace{0.1cm}\\frac{1}{2n}(X\\hat{\\beta}^{OLS} - X\\beta)^{T}X^{T}X(X\\hat{\\beta}^{OLS} - X\\beta) + \\lambda||\\beta||^1$$  \n",
    "\n",
    "$$\\hspace{5cm}Assume\\hspace{0.3cm}we\\hspace{0.3cm}have\\hspace{0.3cm}X^{T}X=nI$$  \n",
    "\n",
    "$$\\hspace{5.8cm}=argmin\\hspace{0.1cm}\\frac{1}{2}(\\hat{\\beta}^{OLS} - \\beta)^{T}(\\hat{\\beta}^{OLS} - \\beta) + \\lambda||\\beta||^1$$  \n",
    "$$\\hspace{5.8cm}=argmin\\hspace{0.1cm}\\frac{1}{2}\\Sigma_{j=1}^{p}(\\beta_{j}^{OLS} - \\beta_{j})^2+ \\lambda||\\beta_{j}||^1$$  \n",
    "\n",
    "**This further implies we can shove the lasso estimateors individually from the OLS estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$For\\hspace{0.1cm}each\\hspace{0.1cm}\\beta_{j}:$$\n",
    "$$\\beta_{j}=argmin\\hspace{0.1cm}\\frac{1}{2}(\\beta - \\hat{\\beta_{jOLS}})^{2}+\\lambda|\\beta|$$  \n",
    "$$Take\\hspace{0.1cm}derivative\\hspace{0.1cm}yields:$$\n",
    "$$\\hat{\\beta_{j}^{lasso}}=(sign\\hspace{0.1cm}\\hat{\\beta_{jOLS}})(|\\hat{\\beta_{jOLS}|-\\lambda})+$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
